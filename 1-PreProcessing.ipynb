{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adbff74c-bad7-48db-8c06-46c8ae5d06e7",
   "metadata": {},
   "source": [
    "# Tutorial 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03f544f-0f47-48c8-97c2-0a3122652135",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382b72f0-283f-428b-bc77-55403b690607",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6e5406-2ec0-42b3-bcef-8c3856cfb7ac",
   "metadata": {},
   "source": [
    "Welcome! This tutorial will show you how to visualise and preprocess astronomical data using python. From this tutorial, you will learn the following:\n",
    "\n",
    "1. How to download astronomical data\n",
    "2. How to read and visualise data\n",
    "3. How to preprocess imagery data\n",
    "\n",
    "The data [GalaxyMNIST](https://github.com/mwalmsley/galaxy_mnist) contains 10,000 images of galaxies (either 3x64x64 or 3x224x224), confidently labelled by Galaxy Zoo volunteers as belonging to one of four morphology classes, where the classes are:\n",
    "\n",
    "1. smooth and round\n",
    "2. smooth and cigar-shaped\n",
    "3. edge-on-disk\n",
    "4. unbarred spiral\n",
    "\n",
    "The galaxies are selected from `Galaxy Zoo DECaLS Campaign` A (GZD-A). The images are shown to volunteers on Galaxy Zoo for them to classify. At least 17 people must have been asked the necessary questions, and at least half of them must have answered with the given class. The class labels are, therefore, much more confident than, for example, simply labelling with the most common answer to some question. For more info, visit this [link](https://github.com/mwalmsley/galaxy_mnist)\n",
    "\n",
    "\n",
    "Labelling data is a big topic in machine learning. Labelled data allow us to use supervised machine learning tools to train on labelled data and classify unlabeled ones. However, labelling data can be costly. Scientists have been trying to figure out a way to label data, but the easiest is that we can depend on human knowledge to label data like the `Galaxy Zoo DECaLS Campaign`.  However, unlabeled data can also be of some benefit if analyzed with the right tools, like unsupervised machine learning. Those tools allow us to explore (find patterns) and cluster the data, leading to discoveries in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9861171f-09cb-4087-8a38-c5efdcc626b6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e74ab73-1be1-4063-b552-dc26466d6390",
   "metadata": {},
   "source": [
    "### Downloading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7505e73e-8fe3-4f89-9299-168c52974f4a",
   "metadata": {},
   "source": [
    "First, let us call the python script that will let us download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0b7a79-6210-47e9-a88e-050230bae66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from galaxy_mnist import GalaxyMNISTHighrez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0a8f9b-705a-4289-a0a3-5f953f3bcad6",
   "metadata": {},
   "source": [
    "'GalaxyMNISTHighrez' class has the follwing attribuits: \n",
    " - `root`: Specifing the dir to download the data\n",
    " - `download:` A boolean value, `True` to download the images\n",
    " - `train:` A boolean value, `True` to assign the just the training data, `False` will assgin just the testing data. \n",
    "   - `Notice:` The data has a fixed 80/20 train/test division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e526b1-fa23-4b61-be7f-e0b69f2fe157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the training data\n",
    "dataset_train = GalaxyMNISTHighrez(\n",
    "    root='data_import/data',\n",
    "    download=True,\n",
    "    train=True  # by default True, or False for canonical test set\n",
    ")\n",
    "# for the testing data\n",
    "dataset_test = GalaxyMNISTHighrez(\n",
    "    root='data_import/data',\n",
    "    download=True,\n",
    "    train=False  # by default True, or False for canonical test set\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c67448b-a84d-4d3b-8c20-4be1c8707f81",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0811f16c-41f5-4576-8346-489b81daeaf5",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8bd2ee-cda3-4eea-a685-45ea0b6ebd7c",
   "metadata": {},
   "source": [
    "first, let us read inputs `images` and outputs `labels` into two seperate variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1681f5-5382-4179-b0f1-b8932018c8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train = dataset_train.data\n",
    "labels_train = dataset_train.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc89ec2-cfed-4756-b595-4ab69eb1bb56",
   "metadata": {},
   "source": [
    "let us check the shape of the two datasets (traning, and testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d453cced-95bd-498a-acc8-83da58636e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the input (training) \"+str(images_train.shape)+ \", type: \"+ str(images_train.dtype))\n",
    "print(\"Shape of the output (training) \"+str(labels_train.shape)+ \", type: \"+ str(labels_train.dtype))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9453e0f1-96c5-4fc6-8d7c-98447cf9893d",
   "metadata": {},
   "source": [
    "we notice the following:\n",
    " 1. The input data has 8000 samples with 3 channels, which stands for `rgb` colors, where each sample has a size of 224\n",
    " 2. the output is just one dimnsional array, which contains the lables of the samples, and as we mentioned above the label can be one of the following: a)  2. the output is just one dimnsional array, which contains the lables of the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52a7bbd-b5d3-4fe3-b8f2-6d62d667dacb",
   "metadata": {},
   "source": [
    "**Exercise 1:**\n",
    "Store the testing set inputs and outputs into two different variables, and display their shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a886e8a-9fab-4d24-8011-c74372955262",
   "metadata": {},
   "outputs": [],
   "source": [
    "### -- Code here --\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3047b3ef-da99-4da4-9aef-34fdb1fad04b",
   "metadata": {},
   "source": [
    "Now, let us check the frequency of the labels/output, but fiest let us orint them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4970e7-3b81-4593-8e82-3f203ad94b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "GalaxyMNISTHighrez.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6030ce59-c41b-4509-a591-9033e2035216",
   "metadata": {},
   "source": [
    "   **Where**:\n",
    "   - 0: `smooth_round`,\n",
    "   - 1: `smooth_cigar`,\n",
    "   - 2: `edge_on_disk`,\n",
    "   - 3: `unbarred_spiral`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059b9266-dff1-421d-9165-b84c92fbc354",
   "metadata": {},
   "source": [
    "to count the frequncy of the labels, we will convert the data into pandas dataframe\n",
    "@@ Ezra, do you think we can do this step better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8112e9c-3754-4287-b5a4-659fdc871f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # for importing data into data frame format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3700c5-40c0-4ee6-adbc-c5226af79703",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labelsTrain = pd.DataFrame(labels_train)\n",
    "dict_train =  df_labelsTrain.stack().value_counts().to_dict()\n",
    "print(dict_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03ac782-2c19-4550-83f8-12932b3be7c3",
   "metadata": {},
   "source": [
    "### Data visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2980958-3242-42f5-93f4-9a02128c1a33",
   "metadata": {},
   "source": [
    "we can also try to visualse label frequencies in a bar graph, but first let us call the package that will allow us to draw the graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d860a861-e2d8-41ee-a73f-a62a5df5f0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt #This displays graphs once they have been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae0b89a-cbca-4d97-8992-02c8253aee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for newKey, oldKey in  zip( GalaxyMNISTHighrez.classes, sorted(dict_train.keys()) ): \n",
    "    dict_train[newKey] = dict_train.pop(oldKey)  # changing the key names\n",
    "\n",
    "\n",
    "plt.bar(range(len(dict_train)), list(dict_train.values()), align='center')\n",
    "plt.xticks(range(len(dict_train)), list(dict_train.keys()))\n",
    "print(\"Dict: \"+str(dict_train))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b030ef-042b-4dc4-aa0b-78ccb6f84a96",
   "metadata": {},
   "source": [
    "**Exercise 2:**\n",
    "Count the frequency for the testing data and visualise it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f80ca70-7f59-4f2e-803a-6dbf2f45039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### -- Code here --\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2437829e-5fa1-4096-b53f-7f93afa15d97",
   "metadata": {},
   "source": [
    "Now let us take a look at the images of the different galaxy shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b8e085-fbe0-485e-8826-00f84ed53a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # For handling N-DIMENSIONAL ARRAYS\n",
    "from PIL import Image #  PIL is the Python Imaging Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ed7cf2-ebed-4547-a236-26eb3618685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class0 = np.where (np.array(labels_train) == 0 )\n",
    "class1 = np.where (np.array(labels_train) == 1 )\n",
    "class2 = np.where (np.array(labels_train) == 2 )\n",
    "class3 = np.where (np.array(labels_train) == 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e6dc8f-2107-4c28-8297-4f71c46f8678",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 1\n",
    "columns = 5\n",
    "for classArr in [class0, class1, class2, class3]:\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    for i in range (5):\n",
    "        train_image, train_label = dataset_train[classArr[0][i]]\n",
    "        fig.add_subplot(rows, columns, i+1)\n",
    "        plt.imshow(train_image)\n",
    "    print(\"label: \"+str(GalaxyMNISTHighrez.classes[train_label]))\n",
    "    plt.tight_layout()\n",
    "    plt.show()     # increase the number of images per class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff98eb7-d589-4c01-a8d2-c6e4f7cb08bc",
   "metadata": {},
   "source": [
    "The resolution of this dataset does not seem to be good, `smooth cigar` and `edge_on_disk` kind of look like each other, which will give us problems later on when performing classification or clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef633f20-c7fe-456a-bca4-e0f63a07e1fb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a2e4fd-e10a-473c-a754-26fe1859a148",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a5fbea-1d8f-4983-898d-f610a09d60b8",
   "metadata": {},
   "source": [
    "Classical machine learning usually requires the data to go under many layers of processing so it can be fit for training (e.g. preprocessing, feature extraction, optimization, ..). For each layer, many different algorithms may be used. \n",
    "\n",
    "On the other hand, deep learning models with enough data are often referred to as End-to-End learning models ([E2E](https://towardsdatascience.com/e2e-the-every-purpose-ml-method-5d4f20dafee4)), since it can extract information from raw data.\n",
    "\n",
    "In the following tutorial, we will use an autoencoder (deep learning) model to extract features. Therefore we can feed it the raw data. So a question arises, why are we preprocessing the data? Well, for two reasons related to time complexity. We need to train Deep learning models in a reasonable time so that we can bring the idea across, but more importantly, tuning the hyperparameters for deep learning models can be very tough; therefore, if we think that we can manually reduce the complexity of the data without losing lots/any information that would be a perfect idea. \n",
    "\n",
    "But How Can we tell if we lost lots of info? In our case, since the data is imagery data, we can notify in two ways.\n",
    "a) if our eyes can manually still classify the images, we can conclude that the preprocessing is a good idea.\n",
    "b) testing the results of the preprocessed data on a validation set and comparing it against the original/raw data.\n",
    "\n",
    "This is a suggested pipeline. However, participants are encouraged to explore.\n",
    "\n",
    "In this tutorial, we will try to reduce the complexity of the data by doing the following:\n",
    "   - Reduce the number of channels from RGB to greyscale.\n",
    "   - Reduce the size of the image by a factor of 4.\n",
    "   - Perform normalization (divide by 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5933e22-fe04-40de-8354-ee15f34bbc96",
   "metadata": {},
   "source": [
    "#### a- [Grayscale](https://www.tutorialspoint.com/pytorch-how-to-convert-an-image-to-grayscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00708585-0753-43c2-b500-6320e25f493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms #Transforms are for common image transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f713795-caae-463e-a2d7-02baf84f32c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16100fa-e332-4a32-937d-cfd3b7f8db06",
   "metadata": {},
   "source": [
    "We can see that the original image size has three channels which stand for the RGB colours, now let us perfom the `greyscalling`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37cd306-8d90-4a3e-8777-e20ed2c42bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformGrey = transforms.Grayscale()\n",
    "images_trainGrey = transformGrey(images_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d97ed52-62c4-4b8f-aa95-59182b7cd51d",
   "metadata": {},
   "source": [
    "Now let us check the size of the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c518854-5fb5-4993-9dc7-2a2a0700bfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_trainGrey.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da834b13-6b0c-4b02-9972-c8f97430cab7",
   "metadata": {},
   "source": [
    "we can see the number of channels went down from 3 to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f92a8b-3463-42d0-bbca-8ebe55a9c163",
   "metadata": {},
   "source": [
    "**Exercise 3:**\n",
    "Visualise the grayscale images for all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ef7e7e-7b0b-4a95-b45e-4b7fda8c90dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### -- Code here__\n",
    "\n",
    "rows = 1\n",
    "columns = 5\n",
    "\n",
    "for classArr in [ [class0,0], [class1,1], [class2,2], [class3,3]]:\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    for i in range (5):\n",
    "        train_image = images_trainGrey[classArr[0][0][i]]\n",
    "        fig.add_subplot(rows, columns, i+1)\n",
    "        plt.imshow(train_image[0],cmap='gray', vmin=0, vmax=255 )\n",
    "    print(\"label: \"+str(GalaxyMNISTHighrez.classes[classArr[1]]))\n",
    "    plt.tight_layout()\n",
    "    plt.show()  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e0c702-ec9a-4838-97f2-dbd560db0e2c",
   "metadata": {},
   "source": [
    "We can notice that visually that nothing changed much from the original data. However, we reduced the complexity of the data by a favour of 3, which is enormous!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f23918-79d1-477b-9e63-0ed5dda9b2b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### b- [Size reduction](https://www.tutorialspoint.com/pytorch-how-to-resize-an-image-to-a-given-size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e01eeee-03d1-4b21-9726-ad7fddc4ebf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformResize = transforms.Resize(56)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d591387e-b384-4c44-b293-65cacec4c41f",
   "metadata": {},
   "source": [
    "Now let us reduce the image, we will reduce it on the grayscale images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e16e25-1fc3-4931-8245-38a6a248567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_trainResized = transformResize(images_trainGrey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63563091-5140-45d5-aad3-f5933ba6dd14",
   "metadata": {},
   "source": [
    "now let us check the size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27581f1b-e728-4e07-b6e3-eda072969701",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_trainResized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96c7abc-b630-4fb7-ba86-4d06c67e9119",
   "metadata": {},
   "source": [
    "**Exercise 4:**\n",
    "Visualise the resized images for all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6228e9-9d8e-49d3-9760-b9794393590f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#___ Code here___\n",
    "\n",
    "for classArr in [ [class0,0], [class1,1], [class2,2], [class3,3]]:\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    for i in range (5):\n",
    "        train_image = images_trainResized[classArr[0][0][i]]\n",
    "        fig.add_subplot(rows, columns, i+1)\n",
    "        plt.imshow(train_image[0],cmap='gray', vmin=0, vmax=255 )\n",
    "    print(\"label: \"+str(GalaxyMNISTHighrez.classes[classArr[1]]))\n",
    "    plt.tight_layout()\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5773d3-aa06-490c-93f2-c5ca52476980",
   "metadata": {},
   "source": [
    "#### c- Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c35d031-dc9a-4642-ab6c-99f4a134adb9",
   "metadata": {},
   "source": [
    "Now we will simply try to devide the pixel values by 255. Where the goal is to transform features to be on a similar scale ([source](https://developers.google.com/machine-learning/data-prep/transform/normalization))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64f75cf-e7a0-4550-93b8-169fbab9d792",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_trainPre = images_trainResized/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885b96e0-2c4d-4848-96e4-b4bfbc25a3cd",
   "metadata": {},
   "source": [
    "Now let us check the values before and after the nomalisatiom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5699e7f-c2e7-484d-b68b-d638735dbc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images_trainResized[0][0][0])\n",
    "print(images_trainPre[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1497cc27-a360-4c37-8417-56216143cfbd",
   "metadata": {},
   "source": [
    "##### **_End of the pre-processing pipeline_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ce2ec6-c0b5-485f-91ab-da9442e6d4fc",
   "metadata": {},
   "source": [
    "Now the above pre-processing is not the only way where we can reduce the features, the following can also be done:\n",
    "1. Cropping images away from the centre\n",
    "2. Image segmentation, maybe to remove background noise (Deep learning can be used for this)\n",
    "3. ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321f4c8b-4269-4f37-93eb-2d995c246274",
   "metadata": {},
   "source": [
    "**Exercise 5**: Perform the pre-processing pipeline but on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6238447f-95bc-4c42-812b-32ba8f51b88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#___ Code here___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dff1771-5ae0-4903-893b-258a96ea2b9f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49edf65d-e9ca-497a-a0fa-e4932faa1d1f",
   "metadata": {},
   "source": [
    "### **_End of Tutorial 1_**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
